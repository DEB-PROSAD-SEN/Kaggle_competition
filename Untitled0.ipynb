{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN06R2vuIbcmL/vrPWsEftj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DEB-PROSAD-SEN/Kaggle_competition/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swk8llFWWwyW",
        "outputId": "747a8c17-2be6-4c87-fd24-a4ae7903c1bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN6Lh-jPW3Gl",
        "outputId": "04b2cc79-57d3-4ba4-ec84-e5fe617215b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from Optuna)\n",
            "  Downloading alembic-1.16.5-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from Optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from Optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from Optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from Optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from Optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from Optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->Optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->Optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->Optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.5-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.4/247.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, Optuna\n",
            "Successfully installed Optuna-4.5.0 alembic-1.16.5 colorlog-6.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dc16ZJ5RUshc"
      },
      "outputs": [],
      "source": [
        "# ==== BLOCK 1: Imports & Setup ====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_predict\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectFromModel, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import Ridge, ElasticNet, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Optional libs (may not be installed)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    lgb = None\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostRegressor\n",
        "except Exception:\n",
        "    CatBoostRegressor = None\n",
        "\n",
        "try:\n",
        "    import optuna\n",
        "except Exception:\n",
        "    optuna = None\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "def rmsle(y_true, y_pred):\n",
        "    y_pred = np.maximum(y_pred, 0)\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
        "\n",
        "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 2: Load & prune ====\n",
        "FILE_PATH = r\"/content/train (1).csv\"  # adjust if needed\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "# Drop columns with >50% missing values (robust baseline)\n",
        "threshold = 0.5 * len(df)\n",
        "df = df.dropna(thresh=threshold, axis=1).copy()\n",
        "\n",
        "target_col = df.columns[-1]    # target is last column (SalePrice)\n",
        "y = df[target_col].values\n",
        "X = df.drop(columns=[target_col])\n",
        "print(\"Data shape:\", X.shape, \"| Target:\", target_col)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iog8D24ZV2aJ",
        "outputId": "aa4483ed-1b5d-4120-9dc4-d82adf0369e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (1460, 75) | Target: SalePrice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 3: Outlier handling (Winsorization) ====\n",
        "class Winsorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, lower=0.01, upper=0.99):\n",
        "        self.lower = lower\n",
        "        self.upper = upper\n",
        "        self.bounds_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        num_cols = X.select_dtypes(exclude=[\"object\"]).columns\n",
        "        for c in num_cols:\n",
        "            lo = X[c].quantile(self.lower)\n",
        "            hi = X[c].quantile(self.upper)\n",
        "            self.bounds_[c] = (lo, hi)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for c, (lo, hi) in self.bounds_.items():\n",
        "            if c in X.columns:\n",
        "                X[c] = X[c].clip(lo, hi)\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "n9tzgAbgV4ZM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 4: Feature Engineering ====\n",
        "\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Adds domain features commonly helpful for Ames dataset. \"\"\"\n",
        "    def __init__(self):\n",
        "        self.lf_by_nbhd_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        if \"LotFrontage\" in X.columns and \"Neighborhood\" in X.columns:\n",
        "            self.lf_by_nbhd_ = X.groupby(\"Neighborhood\")[\"LotFrontage\"].median()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Neighborhood-median impute for LotFrontage\n",
        "        if self.lf_by_nbhd_ is not None and \"LotFrontage\" in X.columns:\n",
        "            mask = X[\"LotFrontage\"].isna()\n",
        "            X.loc[mask, \"LotFrontage\"] = X.loc[mask, \"Neighborhood\"].map(self.lf_by_nbhd_)\n",
        "\n",
        "        def has(col): return col in X.columns\n",
        "\n",
        "        # Core totals & ages\n",
        "        if all(has(c) for c in [\"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\"]):\n",
        "            X[\"TotalSF\"] = X[\"TotalBsmtSF\"].fillna(0) + X[\"1stFlrSF\"].fillna(0) + X[\"2ndFlrSF\"].fillna(0)\n",
        "        if all(has(c) for c in [\"YrSold\",\"YearBuilt\"]):\n",
        "            X[\"HouseAge\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\n",
        "        if all(has(c) for c in [\"YrSold\",\"YearRemodAdd\"]):\n",
        "            X[\"RemodAge\"] = X[\"YrSold\"] - X[\"YearRemodAdd\"]\n",
        "        if all(has(c) for c in [\"YrSold\",\"GarageYrBlt\"]):\n",
        "            X[\"GarageAge\"] = X[\"YrSold\"] - X[\"GarageYrBlt\"].fillna(X.get(\"YearBuilt\", X[\"YrSold\"]))\n",
        "\n",
        "        # Bathrooms\n",
        "        if all(has(c) for c in [\"FullBath\",\"HalfBath\",\"BsmtFullBath\",\"BsmtHalfBath\"]):\n",
        "            X[\"TotalBath\"] = (X[\"FullBath\"].fillna(0) + 0.5*X[\"HalfBath\"].fillna(0) +\n",
        "                              X[\"BsmtFullBath\"].fillna(0) + 0.5*X[\"BsmtHalfBath\"].fillna(0))\n",
        "\n",
        "        # Porches & decks\n",
        "        porch_cols = [c for c in [\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"WoodDeckSF\"] if has(c)]\n",
        "        if porch_cols:\n",
        "            X[\"TotalPorchSF\"] = X[porch_cols].fillna(0).sum(axis=1)\n",
        "\n",
        "        # Flags\n",
        "        if has(\"GarageArea\"):\n",
        "            X[\"HasGarage\"] = (X[\"GarageArea\"].fillna(0) > 0).astype(int)\n",
        "        if has(\"TotalBsmtSF\"):\n",
        "            X[\"HasBsmt\"] = (X[\"TotalBsmtSF\"].fillna(0) > 0).astype(int)\n",
        "        if has(\"Fireplaces\"):\n",
        "            X[\"HasFireplace\"] = (X[\"Fireplaces\"].fillna(0) > 0).astype(int)\n",
        "\n",
        "        # Interactions / Ratios\n",
        "        if has(\"GrLivArea\") and has(\"OverallQual\"):\n",
        "            X[\"QualGrLivArea\"] = X[\"GrLivArea\"].fillna(0) * X[\"OverallQual\"].fillna(0)\n",
        "        if has(\"BedroomAbvGr\") and has(\"TotalBath\"):\n",
        "            X[\"BathPerBedroom\"] = X[\"TotalBath\"].replace(0, np.nan) / X[\"BedroomAbvGr\"].replace(0, np.nan)\n",
        "\n",
        "        # Cyclic month features\n",
        "        if \"MoSold\" in X.columns:\n",
        "            X[\"MoSold_sin\"] = np.sin(2*np.pi*X[\"MoSold\"]/12.0)\n",
        "            X[\"MoSold_cos\"] = np.cos(2*np.pi*X[\"MoSold\"]/12.0)\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "class OrdinalMapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Map ordered quality categories to integers. \"\"\"\n",
        "    def __init__(self):\n",
        "        self.qual = {\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5}\n",
        "        self.exposure = {\"None\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4}\n",
        "        self.bsmtfin = {\"None\":0, \"Unf\":1, \"LwQ\":2, \"Rec\":3, \"BLQ\":4, \"ALQ\":5, \"GLQ\":6}\n",
        "        self.paved = {\"N\":0, \"P\":1, \"Y\":2}\n",
        "\n",
        "    def _map(self, X, col, mapping):\n",
        "        if col in X.columns:\n",
        "            X[col] = X[col].fillna(\"None\").map(mapping).astype(\"float64\")\n",
        "\n",
        "    def fit(self, X, y=None): return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for c in [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\n",
        "                  \"HeatingQC\",\"KitchenQual\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]:\n",
        "            self._map(X, c, self.qual)\n",
        "        self._map(X, \"BsmtExposure\", self.exposure)\n",
        "        for c in [\"BsmtFinType1\",\"BsmtFinType2\"]:\n",
        "            self._map(X, c, self.bsmtfin)\n",
        "        self._map(X, \"PavedDrive\", self.paved)\n",
        "        return X\n",
        "\n",
        "\n",
        "class RareGrouper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Replace infrequent categories with 'Rare' to stabilize OHE. \"\"\"\n",
        "    def __init__(self, min_count=20):\n",
        "        self.min_count = min_count\n",
        "        self.keep_: Dict[str, set] = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        for c in X.select_dtypes(include=[\"object\"]).columns:\n",
        "            vc = X[c].value_counts(dropna=False)\n",
        "            self.keep_[c] = set(vc[vc >= self.min_count].index.astype(str))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for c, keep in self.keep_.items():\n",
        "            if c in X.columns:\n",
        "                X[c] = X[c].astype(str)\n",
        "                X.loc[~X[c].isin(keep), c] = \"Rare\"\n",
        "        return X\n",
        "\n",
        "\n",
        "class SkewFixer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Log1p-transform skewed numeric features (except Year*, MoSold, YrSold). \"\"\"\n",
        "    def __init__(self, skew_threshold=0.75):\n",
        "        self.skew_threshold = skew_threshold\n",
        "        self.to_log_: List[str] = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        num_cols = X.select_dtypes(exclude=[\"object\"]).columns\n",
        "        exclude = [c for c in num_cols if c.startswith(\"Year\")] + [\"MoSold\",\"YrSold\"]\n",
        "        cand = [c for c in num_cols if c not in exclude]\n",
        "        skewness = X[cand].fillna(0).apply(lambda s: s.skew())\n",
        "        self.to_log_ = [c for c, sk in skewness.items() if sk > self.skew_threshold]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for c in self.to_log_:\n",
        "            if c in X.columns:\n",
        "                X[c] = np.log1p(X[c].clip(lower=0))\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "ngsf5zXUV66f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 5: Preprocess + Feature Selection ====\n",
        "winsor = Winsorizer(0.01, 0.99)\n",
        "fe = FeatureEngineer()\n",
        "ordmap = OrdinalMapper()\n",
        "rares = RareGrouper(min_count=20)\n",
        "skewfix = SkewFixer(0.75)\n",
        "\n",
        "numeric_proc = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"power\", PowerTransformer(method=\"yeo-johnson\", standardize=False)),\n",
        "])\n",
        "\n",
        "categorical_proc = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "])\n",
        "\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_proc, selector(dtype_exclude=[\"object\"])),\n",
        "        (\"cat\", categorical_proc, selector(dtype_include=[\"object\"])),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# SelectFromModel (L1)\n",
        "l1_selector = SelectFromModel(Lasso(alpha=0.01, max_iter=5000, random_state=RANDOM_STATE), threshold=\"median\")\n",
        "\n",
        "# RFE with Ridge\n",
        "rfe_selector = RFE(estimator=Ridge(alpha=10.0, random_state=RANDOM_STATE), n_features_to_select=200, step=0.2)\n",
        "\n",
        "# Optional PCA (after OHE) — keep moderate components\n",
        "pca = PCA(n_components=200, random_state=RANDOM_STATE)\n",
        "\n",
        "# End-to-end preprocessing with togglable selectors (we'll choose per-model)\n",
        "base_pre = Pipeline(steps=[\n",
        "    (\"winsor\", winsor),\n",
        "    (\"fe\", fe),\n",
        "    (\"ord\", ordmap),\n",
        "    (\"rare\", rares),\n",
        "    (\"skew\", skewfix),\n",
        "    (\"pre\", pre),\n",
        "    # Placeholders to be swapped per model:\n",
        "    # (\"sel\", l1_selector) or (\"sel\", rfe_selector) or (\"sel\", \"passthrough\")\n",
        "    # (\"pca\", pca) or \"passthrough\"\n",
        "])"
      ],
      "metadata": {
        "id": "a2u1V9l_V9AD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 6: Target transform + CV ====\n",
        "# We'll wrap each regressor with TransformedTargetRegressor to train on log1p target\n",
        "def wrap_ttr(reg):\n",
        "    return TransformedTargetRegressor(regressor=reg,\n",
        "                                      func=np.log1p,\n",
        "                                      inverse_func=np.expm1)\n",
        "\n",
        "# StratifiedKFold on binned (log) target for balanced folds\n",
        "y_bins = pd.qcut(np.log1p(y), q=10, labels=False, duplicates=\"drop\")\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n"
      ],
      "metadata": {
        "id": "_no53dazV9yT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 7: Models & grids ====\n",
        "models = {}\n",
        "\n",
        "# Linear / Elastic\n",
        "models[\"Ridge_l1sel\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", l1_selector), (\"reg\", wrap_ttr(Ridge(random_state=RANDOM_STATE)))])\n",
        "models[\"Elastic_rfe\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", rfe_selector), (\"reg\", wrap_ttr(ElasticNet(max_iter=5000, random_state=RANDOM_STATE)))])\n",
        "\n",
        "# Tree ensembles (selector often \"passthrough\")\n",
        "models[\"RF\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"), (\"reg\", wrap_ttr(RandomForestRegressor(random_state=RANDOM_STATE)) )])\n",
        "models[\"GB\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"), (\"reg\", wrap_ttr(GradientBoostingRegressor(random_state=RANDOM_STATE)) )])\n",
        "\n",
        "# Optional boosters\n",
        "if xgb is not None:\n",
        "    models[\"XGB\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"),\n",
        "                              (\"reg\", wrap_ttr(xgb.XGBRegressor(\n",
        "                                  objective=\"reg:squarederror\", random_state=RANDOM_STATE, tree_method=\"hist\"\n",
        "                              )))])\n",
        "if lgb is not None:\n",
        "    models[\"LGBM\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"),\n",
        "                               (\"reg\", wrap_ttr(lgb.LGBMRegressor(random_state=RANDOM_STATE)))])\n",
        "if CatBoostRegressor is not None:\n",
        "    models[\"CAT\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"),\n",
        "                              (\"reg\", wrap_ttr(CatBoostRegressor(\n",
        "                                  loss_function=\"RMSE\", random_state=RANDOM_STATE, verbose=0\n",
        "                              )))])\n"
      ],
      "metadata": {
        "id": "61NLTuj5WLzP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 8: Hyperparameter tuning ====\n",
        "results = {}\n",
        "best_pipes = {}\n",
        "\n",
        "def evaluate_pipe(name, pipe, param_grid=None, search=\"grid\", n_iter=25):\n",
        "    if param_grid is None:\n",
        "        # default safe params\n",
        "        param_grid = {}\n",
        "    if search == \"random\":\n",
        "        searcher = RandomizedSearchCV(pipe, param_distributions=param_grid, n_iter=n_iter,\n",
        "                                      scoring=rmsle_scorer, cv=cv, n_jobs=-1, verbose=1, random_state=RANDOM_STATE)\n",
        "    else:\n",
        "        searcher = GridSearchCV(pipe, param_grid=param_grid, scoring=rmsle_scorer, cv=cv, n_jobs=-1, verbose=1)\n",
        "    searcher.fit(X, y_bins)  # use y_bins only for CV splitting; scoring uses y internally\n",
        "    # we need to refit with original y for scoring — GridSearchCV already refit on full X,y (wrapped TTR handles y)\n",
        "    best_rmsle = -searcher.best_score_\n",
        "    results[name] = {\"Best RMSLE\": best_rmsle, \"Best Params\": searcher.best_params_}\n",
        "    best_pipes[name] = searcher.best_estimator_\n",
        "\n",
        "# Param grids\n",
        "grid_ridge = {\n",
        "    \"sel\": [l1_selector, \"passthrough\"],\n",
        "    \"reg__regressor__alpha\": [0.1, 1.0, 10.0, 50.0],\n",
        "}\n",
        "grid_elastic = {\n",
        "    \"sel\": [rfe_selector, l1_selector, \"passthrough\"],\n",
        "    \"reg__regressor__alpha\": [0.001, 0.01, 0.1, 1.0],\n",
        "    \"reg__regressor__l1_ratio\": [0.2, 0.5, 0.8],\n",
        "}\n",
        "grid_rf = {\n",
        "    \"reg__regressor__n_estimators\": [400, 700],\n",
        "    \"reg__regressor__max_depth\": [None, 12, 20],\n",
        "    \"reg__regressor__min_samples_leaf\": [1, 3, 5],\n",
        "}\n",
        "grid_gb = {\n",
        "    \"reg__regressor__n_estimators\": [400, 800],\n",
        "    \"reg__regressor__learning_rate\": [0.03, 0.05, 0.1],\n",
        "    \"reg__regressor__max_depth\": [2, 3, 5],\n",
        "    \"reg__regressor__subsample\": [0.8, 1.0],\n",
        "    \"reg__regressor__min_samples_leaf\": [1, 3, 5],\n",
        "}\n",
        "\n",
        "# Run searches\n",
        "evaluate_pipe(\"Ridge_l1sel\", models[\"Ridge_l1sel\"], grid_ridge, search=\"grid\")\n",
        "evaluate_pipe(\"Elastic_rfe\", models[\"Elastic_rfe\"], grid_elastic, search=\"grid\")\n",
        "evaluate_pipe(\"RF\", models[\"RF\"], grid_rf, search=\"random\", n_iter=16)\n",
        "evaluate_pipe(\"GB\", models[\"GB\"], grid_gb, search=\"grid\")\n",
        "\n",
        "# XGB/LGBM/CAT\n",
        "if xgb is not None:\n",
        "    grid_xgb = {\n",
        "        \"reg__regressor__n_estimators\": [800, 1200],\n",
        "        \"reg__regressor__max_depth\": [3, 4, 5],\n",
        "        \"reg__regressor__learning_rate\": [0.03, 0.05, 0.1],\n",
        "        \"reg__regressor__subsample\": [0.8, 1.0],\n",
        "        \"reg__regressor__colsample_bytree\": [0.6, 0.8, 1.0]\n",
        "    }\n",
        "    evaluate_pipe(\"XGB\", models[\"XGB\"], grid_xgb, search=\"random\", n_iter=18)\n",
        "\n",
        "if lgb is not None and optuna is not None:\n",
        "    # Optuna tuning for LGBM\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 600, 1400),\n",
        "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 64),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.12),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 25),\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 0.5),\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 0.5),\n",
        "            \"random_state\": RANDOM_STATE\n",
        "        }\n",
        "        pipe = clone(models[\"LGBM\"])\n",
        "        # Correctly set parameters for the regressor within the pipeline\n",
        "        pipe.set_params(**{\"reg__regressor__\" + key: value for key, value in params.items()})\n",
        "        # cross_val_predict for scoring with original y\n",
        "        preds = cross_val_predict(pipe, X, y, cv=cv, n_jobs=-1, verbose=0, method=\"predict\")\n",
        "        return rmsle(y, preds)\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
        "    lgb_best = clone(models[\"LGBM\"]).set_params(**{\"reg__regressor__\" + key: value for key, value in study.best_params.items()})\n",
        "    preds = cross_val_predict(lgb_best, X, y, cv=cv, n_jobs=-1, verbose=0, method=\"predict\")\n",
        "    results[\"LGBM_Optuna\"] = {\"Best RMSLE\": rmsle(y, preds), \"Best Params\": study.best_params}\n",
        "    best_pipes[\"LGBM_Optuna\"] = lgb_best\n",
        "elif lgb is not None:\n",
        "    grid_lgb = {\n",
        "        \"reg__regressor__n_estimators\": [800, 1200],\n",
        "        \"reg__regressor__learning_rate\": [0.03, 0.06, 0.1],\n",
        "        \"reg__regressor__max_depth\": [3, 5, 7],\n",
        "        \"reg__regressor__num_leaves\": [31, 47, 63],\n",
        "        \"reg__regressor__subsample\": [0.8, 1.0],\n",
        "        \"reg__regressor__colsample_bytree\": [0.7, 1.0]\n",
        "    }\n",
        "    evaluate_pipe(\"LGBM\", models[\"LGBM\"], grid_lgb, search=\"random\", n_iter=18)\n",
        "\n",
        "if CatBoostRegressor is not None:\n",
        "    grid_cat = {\n",
        "        \"reg__regressor__depth\": [4, 6, 8],\n",
        "        \"reg__regressor__learning_rate\": [0.03, 0.06, 0.1],\n",
        "        \"reg__regressor__n_estimators\": [800, 1200],\n",
        "        \"reg__regressor__l2_leaf_reg\": [3, 5, 7],\n",
        "    }\n",
        "    evaluate_pipe(\"CAT\", models[\"CAT\"], grid_cat, search=\"random\", n_iter=16)\n",
        "\n",
        "# Show tuning summary\n",
        "summary = (pd.DataFrame(results).T).sort_values(\"Best RMSLE\")\n",
        "print(\"\\n==== MODEL RANKING (lower RMSLE is better) ====\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "m3OphEq9WOSD",
        "outputId": "e22584b4-5d5e-446d-f017-0b058538c815"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-09-04 05:37:27,449] A new study created in memory with name: no-name-1f4b5277-c3dd-40ba-a6be-fffc0c1eb42d\n",
            "[I 2025-09-04 05:37:38,626] Trial 0 finished with value: 0.1333867703011337 and parameters: {'n_estimators': 918, 'num_leaves': 28, 'max_depth': 5, 'learning_rate': 0.06630170697686288, 'subsample': 0.9176418328209136, 'colsample_bytree': 0.7470888430937245, 'min_child_samples': 11, 'reg_alpha': 0.235476467043213, 'reg_lambda': 0.05075143708537855}. Best is trial 0 with value: 0.1333867703011337.\n",
            "[I 2025-09-04 05:37:46,536] Trial 1 finished with value: 0.13272782402134475 and parameters: {'n_estimators': 761, 'num_leaves': 45, 'max_depth': 6, 'learning_rate': 0.028453093798283573, 'subsample': 0.7318328938654294, 'colsample_bytree': 0.7228288538233227, 'min_child_samples': 13, 'reg_alpha': 0.10326643841585331, 'reg_lambda': 0.18842017523302546}. Best is trial 1 with value: 0.13272782402134475.\n",
            "[I 2025-09-04 05:37:56,754] Trial 2 finished with value: 0.13276006768048534 and parameters: {'n_estimators': 1218, 'num_leaves': 47, 'max_depth': 5, 'learning_rate': 0.03514702726942161, 'subsample': 0.9406037740698368, 'colsample_bytree': 0.6238517684631578, 'min_child_samples': 10, 'reg_alpha': 0.23684219321435196, 'reg_lambda': 0.4933525993942054}. Best is trial 1 with value: 0.13272782402134475.\n",
            "[I 2025-09-04 05:38:04,387] Trial 3 finished with value: 0.13443722742321665 and parameters: {'n_estimators': 757, 'num_leaves': 21, 'max_depth': 7, 'learning_rate': 0.058812708132510355, 'subsample': 0.7842072192035439, 'colsample_bytree': 0.6895032269785395, 'min_child_samples': 21, 'reg_alpha': 0.170653138555876, 'reg_lambda': 0.07445985366520763}. Best is trial 1 with value: 0.13272782402134475.\n",
            "[I 2025-09-04 05:38:12,548] Trial 4 finished with value: 0.13267079251757327 and parameters: {'n_estimators': 1337, 'num_leaves': 19, 'max_depth': 4, 'learning_rate': 0.10650178895769818, 'subsample': 0.7191664473599333, 'colsample_bytree': 0.7429392804029169, 'min_child_samples': 6, 'reg_alpha': 0.05963369284206993, 'reg_lambda': 0.4388717555435518}. Best is trial 4 with value: 0.13267079251757327.\n",
            "[I 2025-09-04 05:38:20,795] Trial 5 finished with value: 0.1385760035748076 and parameters: {'n_estimators': 1061, 'num_leaves': 56, 'max_depth': 7, 'learning_rate': 0.08747020260051876, 'subsample': 0.9151193315231141, 'colsample_bytree': 0.9469821622299699, 'min_child_samples': 7, 'reg_alpha': 0.23514233264699586, 'reg_lambda': 0.416143648512307}. Best is trial 4 with value: 0.13267079251757327.\n",
            "[I 2025-09-04 05:38:27,289] Trial 6 finished with value: 0.13234529629607686 and parameters: {'n_estimators': 632, 'num_leaves': 59, 'max_depth': 7, 'learning_rate': 0.02910313118015411, 'subsample': 0.7834167019900143, 'colsample_bytree': 0.7484725428751642, 'min_child_samples': 24, 'reg_alpha': 0.17903200136231545, 'reg_lambda': 0.43592548985967394}. Best is trial 6 with value: 0.13234529629607686.\n",
            "[I 2025-09-04 05:38:33,537] Trial 7 finished with value: 0.13284870778214247 and parameters: {'n_estimators': 1041, 'num_leaves': 40, 'max_depth': 4, 'learning_rate': 0.06765175771932075, 'subsample': 0.8764632248667131, 'colsample_bytree': 0.6974283085962893, 'min_child_samples': 5, 'reg_alpha': 0.4897025304917993, 'reg_lambda': 0.1051220347652923}. Best is trial 6 with value: 0.13234529629607686.\n",
            "[I 2025-09-04 05:38:44,155] Trial 8 finished with value: 0.13562125447275808 and parameters: {'n_estimators': 1105, 'num_leaves': 45, 'max_depth': 6, 'learning_rate': 0.04888870846199969, 'subsample': 0.8753722687217419, 'colsample_bytree': 0.9757617252010368, 'min_child_samples': 24, 'reg_alpha': 0.046564154831006066, 'reg_lambda': 0.38879960346875836}. Best is trial 6 with value: 0.13234529629607686.\n",
            "[I 2025-09-04 05:38:51,540] Trial 9 finished with value: 0.1380312700213942 and parameters: {'n_estimators': 968, 'num_leaves': 62, 'max_depth': 7, 'learning_rate': 0.11864912028990172, 'subsample': 0.8974664140949089, 'colsample_bytree': 0.8858090499279384, 'min_child_samples': 22, 'reg_alpha': 0.24325903081973665, 'reg_lambda': 0.1397058188757318}. Best is trial 6 with value: 0.13234529629607686.\n",
            "[I 2025-09-04 05:38:59,232] Trial 10 finished with value: 0.13385480574453568 and parameters: {'n_estimators': 614, 'num_leaves': 64, 'max_depth': 8, 'learning_rate': 0.02586109589828992, 'subsample': 0.9959443175214171, 'colsample_bytree': 0.8449912110765214, 'min_child_samples': 18, 'reg_alpha': 0.39551607772780784, 'reg_lambda': 0.3120259617065553}. Best is trial 6 with value: 0.13234529629607686.\n",
            "[I 2025-09-04 05:39:07,649] Trial 11 finished with value: 0.1338513209972607 and parameters: {'n_estimators': 1385, 'num_leaves': 16, 'max_depth': 3, 'learning_rate': 0.09201163823743638, 'subsample': 0.7034823001645157, 'colsample_bytree': 0.7964052053739368, 'min_child_samples': 17, 'reg_alpha': 0.008446111350489206, 'reg_lambda': 0.2996979885101387}. Best is trial 6 with value: 0.13234529629607686.\n",
            "[I 2025-09-04 05:39:14,635] Trial 12 finished with value: 0.13415586845356514 and parameters: {'n_estimators': 1362, 'num_leaves': 30, 'max_depth': 3, 'learning_rate': 0.117842201794191, 'subsample': 0.797764362371449, 'colsample_bytree': 0.7900295784468732, 'min_child_samples': 25, 'reg_alpha': 0.11655833232071947, 'reg_lambda': 0.4954884251257598}. Best is trial 6 with value: 0.13234529629607686.\n",
            "[I 2025-09-04 05:39:22,085] Trial 13 finished with value: 0.13340666355354022 and parameters: {'n_estimators': 1224, 'num_leaves': 33, 'max_depth': 4, 'learning_rate': 0.08872583366593767, 'subsample': 0.7804433676110215, 'colsample_bytree': 0.6229572757848996, 'min_child_samples': 15, 'reg_alpha': 0.3331829960285739, 'reg_lambda': 0.3953841418757682}. Best is trial 6 with value: 0.13234529629607686.\n",
            "[I 2025-09-04 05:39:28,555] Trial 14 finished with value: 0.13712435625733171 and parameters: {'n_estimators': 831, 'num_leaves': 54, 'max_depth': 8, 'learning_rate': 0.1012196359654938, 'subsample': 0.817172535395098, 'colsample_bytree': 0.8473050436554808, 'min_child_samples': 19, 'reg_alpha': 0.1285176400566953, 'reg_lambda': 0.24241925472120884}. Best is trial 6 with value: 0.13234529629607686.\n",
            "[I 2025-09-04 05:39:35,474] Trial 15 finished with value: 0.13138057109053228 and parameters: {'n_estimators': 626, 'num_leaves': 36, 'max_depth': 4, 'learning_rate': 0.05233437034458947, 'subsample': 0.7320880065088631, 'colsample_bytree': 0.7595644376190693, 'min_child_samples': 8, 'reg_alpha': 0.06871545107552164, 'reg_lambda': 0.43185409279372683}. Best is trial 15 with value: 0.13138057109053228.\n",
            "[I 2025-09-04 05:39:41,559] Trial 16 finished with value: 0.13310363656226276 and parameters: {'n_estimators': 602, 'num_leaves': 37, 'max_depth': 5, 'learning_rate': 0.05209776841671364, 'subsample': 0.7482644401571542, 'colsample_bytree': 0.6540844731855001, 'min_child_samples': 10, 'reg_alpha': 0.17963283639580552, 'reg_lambda': 0.33319062141595}. Best is trial 15 with value: 0.13138057109053228.\n",
            "[I 2025-09-04 05:39:49,749] Trial 17 finished with value: 0.1343915004996578 and parameters: {'n_estimators': 692, 'num_leaves': 51, 'max_depth': 6, 'learning_rate': 0.0404268961195994, 'subsample': 0.8310679060300828, 'colsample_bytree': 0.7713268444787938, 'min_child_samples': 14, 'reg_alpha': 0.3379147961289115, 'reg_lambda': 0.34626682577240603}. Best is trial 15 with value: 0.13138057109053228.\n",
            "[I 2025-09-04 05:39:56,765] Trial 18 finished with value: 0.13177341644888868 and parameters: {'n_estimators': 870, 'num_leaves': 25, 'max_depth': 4, 'learning_rate': 0.020476238471734125, 'subsample': 0.7562288230009501, 'colsample_bytree': 0.8421126405761338, 'min_child_samples': 8, 'reg_alpha': 0.17423190050712284, 'reg_lambda': 0.45149860356661364}. Best is trial 15 with value: 0.13138057109053228.\n",
            "[I 2025-09-04 05:40:03,961] Trial 19 finished with value: 0.13247720890763146 and parameters: {'n_estimators': 894, 'num_leaves': 25, 'max_depth': 3, 'learning_rate': 0.02123996212411812, 'subsample': 0.752348446612977, 'colsample_bytree': 0.9114043641929032, 'min_child_samples': 8, 'reg_alpha': 0.009928398315957175, 'reg_lambda': 0.25787251848227644}. Best is trial 15 with value: 0.13138057109053228.\n",
            "[I 2025-09-04 05:40:09,611] Trial 20 finished with value: 0.1337258584473275 and parameters: {'n_estimators': 824, 'num_leaves': 35, 'max_depth': 4, 'learning_rate': 0.07595051599284443, 'subsample': 0.7544861932531232, 'colsample_bytree': 0.8365139923663835, 'min_child_samples': 8, 'reg_alpha': 0.30340369013485713, 'reg_lambda': 0.4702453683635561}. Best is trial 15 with value: 0.13138057109053228.\n",
            "[I 2025-09-04 05:40:18,227] Trial 21 finished with value: 0.13400110682738153 and parameters: {'n_estimators': 699, 'num_leaves': 41, 'max_depth': 5, 'learning_rate': 0.03972456166928387, 'subsample': 0.7688753726921439, 'colsample_bytree': 0.8246181260739417, 'min_child_samples': 12, 'reg_alpha': 0.17454829086550913, 'reg_lambda': 0.4386587994588052}. Best is trial 15 with value: 0.13138057109053228.\n",
            "[I 2025-09-04 05:40:23,674] Trial 22 finished with value: 0.1312411228238995 and parameters: {'n_estimators': 674, 'num_leaves': 26, 'max_depth': 4, 'learning_rate': 0.03353672432446954, 'subsample': 0.8016478005345419, 'colsample_bytree': 0.8891889722278352, 'min_child_samples': 16, 'reg_alpha': 0.07906970453489598, 'reg_lambda': 0.37591617615928635}. Best is trial 22 with value: 0.1312411228238995.\n",
            "[I 2025-09-04 05:40:30,649] Trial 23 finished with value: 0.13230130561231324 and parameters: {'n_estimators': 693, 'num_leaves': 24, 'max_depth': 4, 'learning_rate': 0.04873899740162381, 'subsample': 0.8171891114031974, 'colsample_bytree': 0.8913790547772368, 'min_child_samples': 15, 'reg_alpha': 0.06763035390132174, 'reg_lambda': 0.37207877236582215}. Best is trial 22 with value: 0.1312411228238995.\n",
            "[I 2025-09-04 05:40:36,194] Trial 24 finished with value: 0.1317974870720903 and parameters: {'n_estimators': 794, 'num_leaves': 30, 'max_depth': 3, 'learning_rate': 0.03616494648943928, 'subsample': 0.8443719921221473, 'colsample_bytree': 0.9317541752173724, 'min_child_samples': 9, 'reg_alpha': 0.09334337340146451, 'reg_lambda': 0.3672811077584649}. Best is trial 22 with value: 0.1312411228238995.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1629626652.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;34m\"reg__regressor__l2_leaf_reg\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     }\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mevaluate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CAT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Show tuning summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1629626652.py\u001b[0m in \u001b[0;36mevaluate_pipe\u001b[0;34m(name, pipe, param_grid, search, n_iter)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrmsle_scorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_bins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# use y_bins only for CV splitting; scoring uses y internally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# we need to refit with original y for scoring — GridSearchCV already refit on full X,y (wrapped TTR handles y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbest_rmsle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 9: Stacking the best models ====\n",
        "# pick top 3-4 models by RMSLE\n",
        "top_names = list((pd.DataFrame(results).T).sort_values(\"Best RMSLE\").index)[:4]\n",
        "estimators = [(n, best_pipes[n]) for n in top_names if n in best_pipes]\n",
        "\n",
        "# Meta model (Ridge on log target via TTR)\n",
        "meta = wrap_ttr(Ridge(alpha=1.0, random_state=RANDOM_STATE))\n",
        "\n",
        "stack = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta,\n",
        "    passthrough=False,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# CV estimate for stack\n",
        "stack_preds = cross_val_predict(stack, X, y, cv=cv, n_jobs=-1, verbose=0, method=\"predict\")\n",
        "stack_rmsle = rmsle(y, stack_preds)\n",
        "print(f\"\\nSTACK ({', '.join([n for n,_ in estimators])}) RMSLE (CV): {stack_rmsle:.6f}\")\n"
      ],
      "metadata": {
        "id": "ubpW5wQzWP_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 10: Final fit & quick predictions ====\n",
        "# choose the single best model or the stack (whichever is better)\n",
        "best_single_name = summary.index[0]\n",
        "best_single_score = summary.iloc[0][\"Best RMSLE\"]\n",
        "use_stack = stack_rmsle < best_single_score\n",
        "\n",
        "if use_stack:\n",
        "    final_model = stack\n",
        "    model_name = \"STACK\"\n",
        "    final_model.fit(X, y)\n",
        "else:\n",
        "    model_name = best_single_name\n",
        "    final_model = best_pipes[best_single_name]\n",
        "    final_model.fit(X, y)\n",
        "\n",
        "print(f\"\\nFinal chosen model: {model_name}\")\n",
        "preds_train = final_model.predict(X)\n",
        "print(\"First 10 predictions:\", np.round(preds_train[:10], 2))\n",
        "print(\"Final RMSLE on full training (optimistic):\", rmsle(y, preds_train))\n"
      ],
      "metadata": {
        "id": "ovUqCxNjWTLj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}