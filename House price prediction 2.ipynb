{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsGJt82tQ6k6gsyKbt51lb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DEB-PROSAD-SEN/Kaggle_competition/blob/main/House%20price%20prediction%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swk8llFWWwyW",
        "outputId": "65b6feb4-a1e9-477e-e4a3-52649deeabfd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wN6Lh-jPW3Gl",
        "outputId": "263867d9-1a8f-4310-a875-3bf93dddef68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Optuna in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from Optuna) (1.16.5)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from Optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from Optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from Optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from Optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from Optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from Optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->Optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->Optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->Optuna) (3.2.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dc16ZJ5RUshc"
      },
      "outputs": [],
      "source": [
        "# ==== BLOCK 1: Imports & Setup ====\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, clone\n",
        "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_predict\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PowerTransformer, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectFromModel, RFE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import Ridge, ElasticNet, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.compose import TransformedTargetRegressor\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Optional libs (may not be installed)\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "except Exception:\n",
        "    lgb = None\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostRegressor\n",
        "except Exception:\n",
        "    CatBoostRegressor = None\n",
        "\n",
        "try:\n",
        "    import optuna\n",
        "except Exception:\n",
        "    optuna = None\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "def rmsle(y_true, y_pred):\n",
        "    y_pred = np.maximum(y_pred, 0)\n",
        "    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n",
        "\n",
        "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 2: Load & prune ====\n",
        "FILE_PATH = r\"/content/train (1).csv\"  # adjust if needed\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "\n",
        "# Drop columns with >50% missing values (robust baseline)\n",
        "threshold = 0.5 * len(df)\n",
        "df = df.dropna(thresh=threshold, axis=1).copy()\n",
        "\n",
        "target_col = df.columns[-1]    # target is last column (SalePrice)\n",
        "y = df[target_col].values\n",
        "X = df.drop(columns=[target_col])\n",
        "print(\"Data shape:\", X.shape, \"| Target:\", target_col)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iog8D24ZV2aJ",
        "outputId": "19e95a45-873c-40fe-adc8-db17e744ca0a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (1460, 75) | Target: SalePrice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 3: Outlier handling (Winsorization) ====\n",
        "class Winsorizer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, lower=0.01, upper=0.99):\n",
        "        self.lower = lower\n",
        "        self.upper = upper\n",
        "        self.bounds_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        num_cols = X.select_dtypes(exclude=[\"object\"]).columns\n",
        "        for c in num_cols:\n",
        "            lo = X[c].quantile(self.lower)\n",
        "            hi = X[c].quantile(self.upper)\n",
        "            self.bounds_[c] = (lo, hi)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for c, (lo, hi) in self.bounds_.items():\n",
        "            if c in X.columns:\n",
        "                X[c] = X[c].clip(lo, hi)\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "n9tzgAbgV4ZM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 4: Feature Engineering ====\n",
        "\n",
        "class FeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Adds domain features commonly helpful for Ames dataset. \"\"\"\n",
        "    def __init__(self):\n",
        "        self.lf_by_nbhd_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        if \"LotFrontage\" in X.columns and \"Neighborhood\" in X.columns:\n",
        "            self.lf_by_nbhd_ = X.groupby(\"Neighborhood\")[\"LotFrontage\"].median()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Neighborhood-median impute for LotFrontage\n",
        "        if self.lf_by_nbhd_ is not None and \"LotFrontage\" in X.columns:\n",
        "            mask = X[\"LotFrontage\"].isna()\n",
        "            X.loc[mask, \"LotFrontage\"] = X.loc[mask, \"Neighborhood\"].map(self.lf_by_nbhd_)\n",
        "\n",
        "        def has(col): return col in X.columns\n",
        "\n",
        "        # Core totals & ages\n",
        "        if all(has(c) for c in [\"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\"]):\n",
        "            X[\"TotalSF\"] = X[\"TotalBsmtSF\"].fillna(0) + X[\"1stFlrSF\"].fillna(0) + X[\"2ndFlrSF\"].fillna(0)\n",
        "        if all(has(c) for c in [\"YrSold\",\"YearBuilt\"]):\n",
        "            X[\"HouseAge\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\n",
        "        if all(has(c) for c in [\"YrSold\",\"YearRemodAdd\"]):\n",
        "            X[\"RemodAge\"] = X[\"YrSold\"] - X[\"YearRemodAdd\"]\n",
        "        if all(has(c) for c in [\"YrSold\",\"GarageYrBlt\"]):\n",
        "            X[\"GarageAge\"] = X[\"YrSold\"] - X[\"GarageYrBlt\"].fillna(X.get(\"YearBuilt\", X[\"YrSold\"]))\n",
        "\n",
        "        # Bathrooms\n",
        "        if all(has(c) for c in [\"FullBath\",\"HalfBath\",\"BsmtFullBath\",\"BsmtHalfBath\"]):\n",
        "            X[\"TotalBath\"] = (X[\"FullBath\"].fillna(0) + 0.5*X[\"HalfBath\"].fillna(0) +\n",
        "                              X[\"BsmtFullBath\"].fillna(0) + 0.5*X[\"BsmtHalfBath\"].fillna(0))\n",
        "\n",
        "        # Porches & decks\n",
        "        porch_cols = [c for c in [\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\",\"WoodDeckSF\"] if has(c)]\n",
        "        if porch_cols:\n",
        "            X[\"TotalPorchSF\"] = X[porch_cols].fillna(0).sum(axis=1)\n",
        "\n",
        "        # Flags\n",
        "        if has(\"GarageArea\"):\n",
        "            X[\"HasGarage\"] = (X[\"GarageArea\"].fillna(0) > 0).astype(int)\n",
        "        if has(\"TotalBsmtSF\"):\n",
        "            X[\"HasBsmt\"] = (X[\"TotalBsmtSF\"].fillna(0) > 0).astype(int)\n",
        "        if has(\"Fireplaces\"):\n",
        "            X[\"HasFireplace\"] = (X[\"Fireplaces\"].fillna(0) > 0).astype(int)\n",
        "\n",
        "        # Interactions / Ratios\n",
        "        if has(\"GrLivArea\") and has(\"OverallQual\"):\n",
        "            X[\"QualGrLivArea\"] = X[\"GrLivArea\"].fillna(0) * X[\"OverallQual\"].fillna(0)\n",
        "        if has(\"BedroomAbvGr\") and has(\"TotalBath\"):\n",
        "            X[\"BathPerBedroom\"] = X[\"TotalBath\"].replace(0, np.nan) / X[\"BedroomAbvGr\"].replace(0, np.nan)\n",
        "\n",
        "        # Cyclic month features\n",
        "        if \"MoSold\" in X.columns:\n",
        "            X[\"MoSold_sin\"] = np.sin(2*np.pi*X[\"MoSold\"]/12.0)\n",
        "            X[\"MoSold_cos\"] = np.cos(2*np.pi*X[\"MoSold\"]/12.0)\n",
        "\n",
        "        return X\n",
        "\n",
        "\n",
        "class OrdinalMapper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Map ordered quality categories to integers. \"\"\"\n",
        "    def __init__(self):\n",
        "        self.qual = {\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5}\n",
        "        self.exposure = {\"None\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4}\n",
        "        self.bsmtfin = {\"None\":0, \"Unf\":1, \"LwQ\":2, \"Rec\":3, \"BLQ\":4, \"ALQ\":5, \"GLQ\":6}\n",
        "        self.paved = {\"N\":0, \"P\":1, \"Y\":2}\n",
        "\n",
        "    def _map(self, X, col, mapping):\n",
        "        if col in X.columns:\n",
        "            X[col] = X[col].fillna(\"None\").map(mapping).astype(\"float64\")\n",
        "\n",
        "    def fit(self, X, y=None): return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for c in [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\n",
        "                  \"HeatingQC\",\"KitchenQual\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]:\n",
        "            self._map(X, c, self.qual)\n",
        "        self._map(X, \"BsmtExposure\", self.exposure)\n",
        "        for c in [\"BsmtFinType1\",\"BsmtFinType2\"]:\n",
        "            self._map(X, c, self.bsmtfin)\n",
        "        self._map(X, \"PavedDrive\", self.paved)\n",
        "        return X\n",
        "\n",
        "\n",
        "class RareGrouper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Replace infrequent categories with 'Rare' to stabilize OHE. \"\"\"\n",
        "    def __init__(self, min_count=20):\n",
        "        self.min_count = min_count\n",
        "        self.keep_: Dict[str, set] = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        for c in X.select_dtypes(include=[\"object\"]).columns:\n",
        "            vc = X[c].value_counts(dropna=False)\n",
        "            self.keep_[c] = set(vc[vc >= self.min_count].index.astype(str))\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for c, keep in self.keep_.items():\n",
        "            if c in X.columns:\n",
        "                X[c] = X[c].astype(str)\n",
        "                X.loc[~X[c].isin(keep), c] = \"Rare\"\n",
        "        return X\n",
        "\n",
        "\n",
        "class SkewFixer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\" Log1p-transform skewed numeric features (except Year*, MoSold, YrSold). \"\"\"\n",
        "    def __init__(self, skew_threshold=0.75):\n",
        "        self.skew_threshold = skew_threshold\n",
        "        self.to_log_: List[str] = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X = X.copy()\n",
        "        num_cols = X.select_dtypes(exclude=[\"object\"]).columns\n",
        "        exclude = [c for c in num_cols if c.startswith(\"Year\")] + [\"MoSold\",\"YrSold\"]\n",
        "        cand = [c for c in num_cols if c not in exclude]\n",
        "        skewness = X[cand].fillna(0).apply(lambda s: s.skew())\n",
        "        self.to_log_ = [c for c, sk in skewness.items() if sk > self.skew_threshold]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        for c in self.to_log_:\n",
        "            if c in X.columns:\n",
        "                X[c] = np.log1p(X[c].clip(lower=0))\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "ngsf5zXUV66f"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 5: Preprocess + Feature Selection ====\n",
        "winsor = Winsorizer(0.01, 0.99)\n",
        "fe = FeatureEngineer()\n",
        "ordmap = OrdinalMapper()\n",
        "rares = RareGrouper(min_count=20)\n",
        "skewfix = SkewFixer(0.75)\n",
        "\n",
        "numeric_proc = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"power\", PowerTransformer(method=\"yeo-johnson\", standardize=False)),\n",
        "])\n",
        "\n",
        "categorical_proc = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "])\n",
        "\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_proc, selector(dtype_exclude=[\"object\"])),\n",
        "        (\"cat\", categorical_proc, selector(dtype_include=[\"object\"])),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False\n",
        ")\n",
        "\n",
        "# SelectFromModel (L1)\n",
        "l1_selector = SelectFromModel(Lasso(alpha=0.01, max_iter=5000, random_state=RANDOM_STATE), threshold=\"median\")\n",
        "\n",
        "# RFE with Ridge\n",
        "rfe_selector = RFE(estimator=Ridge(alpha=10.0, random_state=RANDOM_STATE), n_features_to_select=200, step=0.2)\n",
        "\n",
        "# Optional PCA (after OHE) — keep moderate components\n",
        "pca = PCA(n_components=200, random_state=RANDOM_STATE)\n",
        "\n",
        "# End-to-end preprocessing with togglable selectors (we'll choose per-model)\n",
        "base_pre = Pipeline(steps=[\n",
        "    (\"winsor\", winsor),\n",
        "    (\"fe\", fe),\n",
        "    (\"ord\", ordmap),\n",
        "    (\"rare\", rares),\n",
        "    (\"skew\", skewfix),\n",
        "    (\"pre\", pre),\n",
        "    # Placeholders to be swapped per model:\n",
        "    # (\"sel\", l1_selector) or (\"sel\", rfe_selector) or (\"sel\", \"passthrough\")\n",
        "    # (\"pca\", pca) or \"passthrough\"\n",
        "])"
      ],
      "metadata": {
        "id": "a2u1V9l_V9AD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 6: Target transform + CV ====\n",
        "# We'll wrap each regressor with TransformedTargetRegressor to train on log1p target\n",
        "def wrap_ttr(reg):\n",
        "    return TransformedTargetRegressor(regressor=reg,\n",
        "                                      func=np.log1p,\n",
        "                                      inverse_func=np.expm1)\n",
        "\n",
        "# StratifiedKFold on binned (log) target for balanced folds\n",
        "y_bins = pd.qcut(np.log1p(y), q=10, labels=False, duplicates=\"drop\")\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n"
      ],
      "metadata": {
        "id": "_no53dazV9yT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 7: Models & grids ====\n",
        "models = {}\n",
        "\n",
        "# Linear / Elastic\n",
        "models[\"Ridge_l1sel\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", l1_selector), (\"reg\", wrap_ttr(Ridge(random_state=RANDOM_STATE)))])\n",
        "models[\"Elastic_rfe\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", rfe_selector), (\"reg\", wrap_ttr(ElasticNet(max_iter=5000, random_state=RANDOM_STATE)))])\n",
        "\n",
        "# Tree ensembles (selector often \"passthrough\")\n",
        "models[\"RF\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"), (\"reg\", wrap_ttr(RandomForestRegressor(random_state=RANDOM_STATE)) )])\n",
        "models[\"GB\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"), (\"reg\", wrap_ttr(GradientBoostingRegressor(random_state=RANDOM_STATE)) )])\n",
        "\n",
        "# Optional boosters\n",
        "if xgb is not None:\n",
        "    models[\"XGB\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"),\n",
        "                              (\"reg\", wrap_ttr(xgb.XGBRegressor(\n",
        "                                  objective=\"reg:squarederror\", random_state=RANDOM_STATE, tree_method=\"hist\"\n",
        "                              )))])\n",
        "if lgb is not None:\n",
        "    models[\"LGBM\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"),\n",
        "                               (\"reg\", wrap_ttr(lgb.LGBMRegressor(random_state=RANDOM_STATE)))])\n",
        "if CatBoostRegressor is not None:\n",
        "    models[\"CAT\"] = Pipeline([(\"pre\", clone(base_pre)), (\"sel\", \"passthrough\"),\n",
        "                              (\"reg\", wrap_ttr(CatBoostRegressor(\n",
        "                                  loss_function=\"RMSE\", random_state=RANDOM_STATE, verbose=0\n",
        "                              )))])\n"
      ],
      "metadata": {
        "id": "61NLTuj5WLzP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 8: Hyperparameter tuning ====\n",
        "results = {}\n",
        "best_pipes = {}\n",
        "\n",
        "def evaluate_pipe(name, pipe, param_grid=None, search=\"grid\", n_iter=25):\n",
        "    if param_grid is None:\n",
        "        # default safe params\n",
        "        param_grid = {}\n",
        "    if search == \"random\":\n",
        "        searcher = RandomizedSearchCV(pipe, param_distributions=param_grid, n_iter=n_iter,\n",
        "                                      scoring=rmsle_scorer, cv=cv, n_jobs=-1, verbose=1, random_state=RANDOM_STATE)\n",
        "    else:\n",
        "        searcher = GridSearchCV(pipe, param_grid=param_grid, scoring=rmsle_scorer, cv=cv, n_jobs=-1, verbose=1)\n",
        "    searcher.fit(X, y_bins)  # use y_bins only for CV splitting; scoring uses y internally\n",
        "    # we need to refit with original y for scoring — GridSearchCV already refit on full X,y (wrapped TTR handles y)\n",
        "    best_rmsle = -searcher.best_score_\n",
        "    results[name] = {\"Best RMSLE\": best_rmsle, \"Best Params\": searcher.best_params_}\n",
        "    best_pipes[name] = searcher.best_estimator_\n",
        "\n",
        "# Param grids\n",
        "grid_ridge = {\n",
        "    \"sel\": [l1_selector, \"passthrough\"],\n",
        "    \"reg__regressor__alpha\": [0.1, 1.0, 10.0, 50.0],\n",
        "}\n",
        "grid_elastic = {\n",
        "    \"sel\": [rfe_selector, l1_selector, \"passthrough\"],\n",
        "    \"reg__regressor__alpha\": [0.001, 0.01, 0.1, 1.0],\n",
        "    \"reg__regressor__l1_ratio\": [0.2, 0.5, 0.8],\n",
        "}\n",
        "grid_rf = {\n",
        "    \"reg__regressor__n_estimators\": [400, 700],\n",
        "    \"reg__regressor__max_depth\": [None, 12, 20],\n",
        "    \"reg__regressor__min_samples_leaf\": [1, 3, 5],\n",
        "}\n",
        "grid_gb = {\n",
        "    \"reg__regressor__n_estimators\": [400, 800],\n",
        "    \"reg__regressor__learning_rate\": [0.03, 0.05, 0.1],\n",
        "    \"reg__regressor__max_depth\": [2, 3, 5],\n",
        "    \"reg__regressor__subsample\": [0.8, 1.0],\n",
        "    \"reg__regressor__min_samples_leaf\": [1, 3, 5],\n",
        "}\n",
        "\n",
        "# Run searches\n",
        "evaluate_pipe(\"Ridge_l1sel\", models[\"Ridge_l1sel\"], grid_ridge, search=\"grid\")\n",
        "evaluate_pipe(\"Elastic_rfe\", models[\"Elastic_rfe\"], grid_elastic, search=\"grid\")\n",
        "evaluate_pipe(\"RF\", models[\"RF\"], grid_rf, search=\"random\", n_iter=16)\n",
        "evaluate_pipe(\"GB\", models[\"GB\"], grid_gb, search=\"grid\")\n",
        "\n",
        "# XGB/LGBM/CAT\n",
        "if xgb is not None:\n",
        "    grid_xgb = {\n",
        "        \"reg__regressor__n_estimators\": [800, 1200],\n",
        "        \"reg__regressor__max_depth\": [3, 4, 5],\n",
        "        \"reg__regressor__learning_rate\": [0.03, 0.05, 0.1],\n",
        "        \"reg__regressor__subsample\": [0.8, 1.0],\n",
        "        \"reg__regressor__colsample_bytree\": [0.6, 0.8, 1.0]\n",
        "    }\n",
        "    evaluate_pipe(\"XGB\", models[\"XGB\"], grid_xgb, search=\"random\", n_iter=18)\n",
        "\n",
        "if lgb is not None and optuna is not None:\n",
        "    # Optuna tuning for LGBM\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 600, 1400),\n",
        "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 64),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.12),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
        "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 25),\n",
        "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 0.5),\n",
        "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 0.5),\n",
        "            \"random_state\": RANDOM_STATE\n",
        "        }\n",
        "        pipe = clone(models[\"LGBM\"])\n",
        "        # Correctly set parameters for the regressor within the pipeline\n",
        "        pipe.set_params(**{\"reg__regressor__\" + key: value for key, value in params.items()})\n",
        "        # cross_val_predict for scoring with original y\n",
        "        preds = cross_val_predict(pipe, X, y, cv=cv, n_jobs=-1, verbose=0, method=\"predict\")\n",
        "        return rmsle(y, preds)\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=25, show_progress_bar=False)\n",
        "    lgb_best = clone(models[\"LGBM\"]).set_params(**{\"reg__regressor__\" + key: value for key, value in study.best_params.items()})\n",
        "    preds = cross_val_predict(lgb_best, X, y, cv=cv, n_jobs=-1, verbose=0, method=\"predict\")\n",
        "    results[\"LGBM_Optuna\"] = {\"Best RMSLE\": rmsle(y, preds), \"Best Params\": study.best_params}\n",
        "    best_pipes[\"LGBM_Optuna\"] = lgb_best\n",
        "elif lgb is not None:\n",
        "    grid_lgb = {\n",
        "        \"reg__regressor__n_estimators\": [800, 1200],\n",
        "        \"reg__regressor__learning_rate\": [0.03, 0.06, 0.1],\n",
        "        \"reg__regressor__max_depth\": [3, 5, 7],\n",
        "        \"reg__regressor__num_leaves\": [31, 47, 63],\n",
        "        \"reg__regressor__subsample\": [0.8, 1.0],\n",
        "        \"reg__regressor__colsample_bytree\": [0.7, 1.0]\n",
        "    }\n",
        "    evaluate_pipe(\"LGBM\", models[\"LGBM\"], grid_lgb, search=\"random\", n_iter=18)\n",
        "\n",
        "if CatBoostRegressor is not None:\n",
        "    grid_cat = {\n",
        "        \"reg__regressor__depth\": [4, 6, 8],\n",
        "        \"reg__regressor__learning_rate\": [0.03, 0.06, 0.1],\n",
        "        \"reg__regressor__n_estimators\": [800, 1200],\n",
        "        \"reg__regressor__l2_leaf_reg\": [3, 5, 7],\n",
        "    }\n",
        "    evaluate_pipe(\"CAT\", models[\"CAT\"], grid_cat, search=\"random\", n_iter=16)\n",
        "\n",
        "# Show tuning summary\n",
        "summary = (pd.DataFrame(results).T).sort_values(\"Best RMSLE\")\n",
        "print(\"\\n==== MODEL RANKING (lower RMSLE is better) ====\")\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3OphEq9WOSD",
        "outputId": "0ccca5e3-0f1b-4d8a-83e7-4fb3d4a44881"
      },
      "execution_count": 22,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-09-04 14:37:22,682] A new study created in memory with name: no-name-278b0750-a727-437c-85e9-36bcae2b5148\n",
            "[I 2025-09-04 14:37:32,691] Trial 0 finished with value: 0.1357705370165879 and parameters: {'n_estimators': 1001, 'num_leaves': 18, 'max_depth': 6, 'learning_rate': 0.1102019448431262, 'subsample': 0.994793153776766, 'colsample_bytree': 0.7360521790854737, 'min_child_samples': 5, 'reg_alpha': 0.36876671003369427, 'reg_lambda': 0.3945170901736121}. Best is trial 0 with value: 0.1357705370165879.\n",
            "[I 2025-09-04 14:37:42,667] Trial 1 finished with value: 0.13611743415545083 and parameters: {'n_estimators': 883, 'num_leaves': 59, 'max_depth': 8, 'learning_rate': 0.02830572843895605, 'subsample': 0.8199177460650445, 'colsample_bytree': 0.9085486314727231, 'min_child_samples': 12, 'reg_alpha': 0.3270583679868257, 'reg_lambda': 0.019609910502172423}. Best is trial 0 with value: 0.1357705370165879.\n",
            "[I 2025-09-04 14:37:53,654] Trial 2 finished with value: 0.1341701576545262 and parameters: {'n_estimators': 1226, 'num_leaves': 45, 'max_depth': 5, 'learning_rate': 0.023818028339745837, 'subsample': 0.7388462768520768, 'colsample_bytree': 0.9318776890469617, 'min_child_samples': 19, 'reg_alpha': 0.08013114902709984, 'reg_lambda': 0.3296596460772407}. Best is trial 2 with value: 0.1341701576545262.\n",
            "[I 2025-09-04 14:38:01,588] Trial 3 finished with value: 0.1371851185025847 and parameters: {'n_estimators': 1384, 'num_leaves': 28, 'max_depth': 7, 'learning_rate': 0.10802818636057193, 'subsample': 0.7554205374978515, 'colsample_bytree': 0.8098826767033394, 'min_child_samples': 15, 'reg_alpha': 0.2947789429083632, 'reg_lambda': 0.14339724610975663}. Best is trial 2 with value: 0.1341701576545262.\n",
            "[I 2025-09-04 14:38:10,951] Trial 4 finished with value: 0.13375427352384026 and parameters: {'n_estimators': 886, 'num_leaves': 52, 'max_depth': 5, 'learning_rate': 0.024174551743867227, 'subsample': 0.7544198157742307, 'colsample_bytree': 0.84943724672504, 'min_child_samples': 8, 'reg_alpha': 0.3241422562930775, 'reg_lambda': 0.17360236080848596}. Best is trial 4 with value: 0.13375427352384026.\n",
            "[I 2025-09-04 14:38:19,945] Trial 5 finished with value: 0.1353605231063933 and parameters: {'n_estimators': 1205, 'num_leaves': 46, 'max_depth': 7, 'learning_rate': 0.03933189757853864, 'subsample': 0.9999774126824371, 'colsample_bytree': 0.8974992388754982, 'min_child_samples': 15, 'reg_alpha': 0.3687855988859691, 'reg_lambda': 0.13618006891252343}. Best is trial 4 with value: 0.13375427352384026.\n",
            "[I 2025-09-04 14:38:25,903] Trial 6 finished with value: 0.13378549271472134 and parameters: {'n_estimators': 676, 'num_leaves': 26, 'max_depth': 4, 'learning_rate': 0.09070729923370752, 'subsample': 0.9165155341422528, 'colsample_bytree': 0.7301777713472717, 'min_child_samples': 14, 'reg_alpha': 0.18938107242196212, 'reg_lambda': 0.10201462838344816}. Best is trial 4 with value: 0.13375427352384026.\n",
            "[I 2025-09-04 14:38:33,015] Trial 7 finished with value: 0.13554654316422302 and parameters: {'n_estimators': 1384, 'num_leaves': 47, 'max_depth': 5, 'learning_rate': 0.1005740513935424, 'subsample': 0.7983544713716008, 'colsample_bytree': 0.7062518384706272, 'min_child_samples': 14, 'reg_alpha': 0.24718495145929875, 'reg_lambda': 0.20475689544123704}. Best is trial 4 with value: 0.13375427352384026.\n",
            "[I 2025-09-04 14:38:39,254] Trial 8 finished with value: 0.13111156983457609 and parameters: {'n_estimators': 961, 'num_leaves': 21, 'max_depth': 3, 'learning_rate': 0.03853779530558403, 'subsample': 0.9593886592310474, 'colsample_bytree': 0.8297813897571562, 'min_child_samples': 18, 'reg_alpha': 0.17753540735721624, 'reg_lambda': 0.43221177858617227}. Best is trial 8 with value: 0.13111156983457609.\n",
            "[I 2025-09-04 14:38:47,139] Trial 9 finished with value: 0.1328588112842064 and parameters: {'n_estimators': 865, 'num_leaves': 59, 'max_depth': 5, 'learning_rate': 0.03218197002391298, 'subsample': 0.8722913176370539, 'colsample_bytree': 0.7333673231554461, 'min_child_samples': 13, 'reg_alpha': 0.4224430654524769, 'reg_lambda': 0.44529184239536723}. Best is trial 8 with value: 0.13111156983457609.\n",
            "[I 2025-09-04 14:38:51,851] Trial 10 finished with value: 0.13089918618871033 and parameters: {'n_estimators': 639, 'num_leaves': 34, 'max_depth': 3, 'learning_rate': 0.06248132393179895, 'subsample': 0.931670435413214, 'colsample_bytree': 0.6164655449787266, 'min_child_samples': 25, 'reg_alpha': 0.04175643286448083, 'reg_lambda': 0.4903198280032013}. Best is trial 10 with value: 0.13089918618871033.\n",
            "[I 2025-09-04 14:38:57,099] Trial 11 finished with value: 0.13029727194880236 and parameters: {'n_estimators': 648, 'num_leaves': 33, 'max_depth': 3, 'learning_rate': 0.05680017736518084, 'subsample': 0.9231449215435237, 'colsample_bytree': 0.60267223447396, 'min_child_samples': 25, 'reg_alpha': 0.004653876703881454, 'reg_lambda': 0.4983613194591814}. Best is trial 11 with value: 0.13029727194880236.\n",
            "[I 2025-09-04 14:39:01,514] Trial 12 finished with value: 0.13148860854476835 and parameters: {'n_estimators': 616, 'num_leaves': 36, 'max_depth': 3, 'learning_rate': 0.06258769370075419, 'subsample': 0.8976252425825595, 'colsample_bytree': 0.6101717616818414, 'min_child_samples': 25, 'reg_alpha': 0.001732771968541158, 'reg_lambda': 0.48410413698631033}. Best is trial 11 with value: 0.13029727194880236.\n",
            "[I 2025-09-04 14:39:06,813] Trial 13 finished with value: 0.1301720913396531 and parameters: {'n_estimators': 718, 'num_leaves': 35, 'max_depth': 3, 'learning_rate': 0.060639657013131766, 'subsample': 0.9344631011830922, 'colsample_bytree': 0.6004681711156823, 'min_child_samples': 25, 'reg_alpha': 0.0012649433584181087, 'reg_lambda': 0.31894450133671265}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:39:12,858] Trial 14 finished with value: 0.13341028222909743 and parameters: {'n_estimators': 759, 'num_leaves': 32, 'max_depth': 4, 'learning_rate': 0.07707132141958206, 'subsample': 0.8481238847961217, 'colsample_bytree': 0.6678734827081813, 'min_child_samples': 21, 'reg_alpha': 0.11252960593490079, 'reg_lambda': 0.3071991430019945}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:39:18,893] Trial 15 finished with value: 0.1310421690007032 and parameters: {'n_estimators': 761, 'num_leaves': 40, 'max_depth': 4, 'learning_rate': 0.04730839565259068, 'subsample': 0.954082820906197, 'colsample_bytree': 0.6595819776422246, 'min_child_samples': 22, 'reg_alpha': 0.11170165042725962, 'reg_lambda': 0.27545417833678915}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:39:24,479] Trial 16 finished with value: 0.1314074821314253 and parameters: {'n_estimators': 736, 'num_leaves': 39, 'max_depth': 3, 'learning_rate': 0.07538924331477542, 'subsample': 0.8821300579893654, 'colsample_bytree': 0.6545904137502467, 'min_child_samples': 23, 'reg_alpha': 0.03970669516147941, 'reg_lambda': 0.3788212520818742}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:39:32,847] Trial 17 finished with value: 0.13356909025575567 and parameters: {'n_estimators': 1054, 'num_leaves': 25, 'max_depth': 4, 'learning_rate': 0.05380332086822395, 'subsample': 0.9439128375752281, 'colsample_bytree': 0.9913519049219501, 'min_child_samples': 18, 'reg_alpha': 0.1801096590037987, 'reg_lambda': 0.22608409865163603}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:39:38,561] Trial 18 finished with value: 0.1321025047530582 and parameters: {'n_estimators': 808, 'num_leaves': 31, 'max_depth': 3, 'learning_rate': 0.06496886753687725, 'subsample': 0.84382032291521, 'colsample_bytree': 0.7685902396813313, 'min_child_samples': 20, 'reg_alpha': 0.01047736121331565, 'reg_lambda': 0.3525016103651966}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:39:44,687] Trial 19 finished with value: 0.13296901283736207 and parameters: {'n_estimators': 718, 'num_leaves': 16, 'max_depth': 6, 'learning_rate': 0.08578254832741475, 'subsample': 0.9046040773752791, 'colsample_bytree': 0.6085758936638624, 'min_child_samples': 23, 'reg_alpha': 0.0954310353578733, 'reg_lambda': 0.29622130199777236}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:39:50,990] Trial 20 finished with value: 0.1321411088861515 and parameters: {'n_estimators': 608, 'num_leaves': 52, 'max_depth': 4, 'learning_rate': 0.05655514912220045, 'subsample': 0.9683877021644924, 'colsample_bytree': 0.6614671912503245, 'min_child_samples': 10, 'reg_alpha': 0.48930279320158776, 'reg_lambda': 0.40761462753286043}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:39:55,553] Trial 21 finished with value: 0.13112329744243695 and parameters: {'n_estimators': 645, 'num_leaves': 36, 'max_depth': 3, 'learning_rate': 0.07125849135046752, 'subsample': 0.9260174519526312, 'colsample_bytree': 0.6129697270398325, 'min_child_samples': 25, 'reg_alpha': 0.05542706893069363, 'reg_lambda': 0.4856250627449107}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:40:00,465] Trial 22 finished with value: 0.13039719755086385 and parameters: {'n_estimators': 691, 'num_leaves': 33, 'max_depth': 3, 'learning_rate': 0.05002444264563187, 'subsample': 0.9299750709733047, 'colsample_bytree': 0.6342547682391628, 'min_child_samples': 25, 'reg_alpha': 0.0035450123203790568, 'reg_lambda': 0.4874348213041135}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:40:06,797] Trial 23 finished with value: 0.13107869686522008 and parameters: {'n_estimators': 818, 'num_leaves': 41, 'max_depth': 4, 'learning_rate': 0.0477851869934406, 'subsample': 0.8741445872956394, 'colsample_bytree': 0.6973119524598048, 'min_child_samples': 23, 'reg_alpha': 0.0029242894242513043, 'reg_lambda': 0.4466479670475518}. Best is trial 13 with value: 0.1301720913396531.\n",
            "[I 2025-09-04 14:40:11,397] Trial 24 finished with value: 0.13090128755072564 and parameters: {'n_estimators': 705, 'num_leaves': 30, 'max_depth': 3, 'learning_rate': 0.04970534929333208, 'subsample': 0.964014491960405, 'colsample_bytree': 0.6399567464196451, 'min_child_samples': 25, 'reg_alpha': 0.13354889903591505, 'reg_lambda': 0.4445485218844561}. Best is trial 13 with value: 0.1301720913396531.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "\n",
            "==== MODEL RANKING (lower RMSLE is better) ====\n",
            "            Best RMSLE                                        Best Params\n",
            "LGBM_Optuna   0.130172  {'n_estimators': 718, 'num_leaves': 35, 'max_d...\n",
            "CAT           0.244365  {'reg__regressor__n_estimators': 1200, 'reg__r...\n",
            "XGB           0.248817  {'reg__regressor__subsample': 0.8, 'reg__regre...\n",
            "GB            0.251124  {'reg__regressor__learning_rate': 0.05, 'reg__...\n",
            "RF            0.274468  {'reg__regressor__n_estimators': 400, 'reg__re...\n",
            "Elastic_rfe   0.277408  {'reg__regressor__alpha': 0.001, 'reg__regress...\n",
            "Ridge_l1sel   0.278795  {'reg__regressor__alpha': 10.0, 'sel': 'passth...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 9: Stacking the best models ====\n",
        "# pick top 3-4 models by RMSLE\n",
        "top_names = list((pd.DataFrame(results).T).sort_values(\"Best RMSLE\").index)[:4]\n",
        "estimators = [(n, best_pipes[n]) for n in top_names if n in best_pipes]\n",
        "\n",
        "# Meta model (Ridge on log target via TTR)\n",
        "meta = wrap_ttr(Ridge(alpha=1.0, random_state=RANDOM_STATE))\n",
        "\n",
        "stack = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta,\n",
        "    passthrough=False,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# CV estimate for stack\n",
        "stack_preds = cross_val_predict(stack, X, y, cv=cv, n_jobs=-1, verbose=0, method=\"predict\")\n",
        "stack_rmsle = rmsle(y, stack_preds)\n",
        "print(f\"\\nSTACK ({', '.join([n for n,_ in estimators])}) RMSLE (CV): {stack_rmsle:.6f}\")\n"
      ],
      "metadata": {
        "id": "ubpW5wQzWP_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 10: Final fit & quick predictions ====\n",
        "# choose the single best model or the stack (whichever is better)\n",
        "best_single_name = summary.index[0]\n",
        "best_single_score = summary.iloc[0][\"Best RMSLE\"]\n",
        "use_stack = stack_rmsle < best_single_score\n",
        "\n",
        "if use_stack:\n",
        "    final_model = stack\n",
        "    model_name = \"STACK\"\n",
        "    final_model.fit(X, y)\n",
        "else:\n",
        "    model_name = best_single_name\n",
        "    final_model = best_pipes[best_single_name]\n",
        "    final_model.fit(X, y)\n",
        "\n",
        "print(f\"\\nFinal chosen model: {model_name}\")\n",
        "preds_train = final_model.predict(X)\n",
        "print(\"First 10 predictions:\", np.round(preds_train[:10], 2))\n",
        "print(\"Final RMSLE on full training (optimistic):\", rmsle(y, preds_train))\n"
      ],
      "metadata": {
        "id": "ovUqCxNjWTLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== BLOCK 11: Test Prediction & Submission ====\n",
        "# Load test set\n",
        "TEST_FILE_PATH = r\"/content/test (1).csv\"   # adjust path\n",
        "df_test = pd.read_csv(TEST_FILE_PATH)\n",
        "\n",
        "# Keep Id for submission\n",
        "test_ids = df_test[\"Id\"].values if \"Id\" in df_test.columns else np.arange(len(df_test))\n",
        "\n",
        "# Align columns (if some engineered features don't exist in test, pipeline will create them)\n",
        "# Just ensure df_test has the same base structure as training\n",
        "X_test = df_test.copy()\n",
        "\n",
        "# Predict using the trained pipeline\n",
        "test_preds = final_model.predict(X_test)\n",
        "\n",
        "# Clip to avoid negatives (for prices)\n",
        "test_preds = np.maximum(test_preds, 0)\n",
        "\n",
        "# Build submission DataFrame\n",
        "sub = pd.DataFrame({\n",
        "    \"Id\": test_ids,\n",
        "    target_col: np.round(test_preds, 2)   # round if needed\n",
        "})\n",
        "\n",
        "# Save submission file\n",
        "OUTPUT_PATH = \"submission.csv\"\n",
        "sub.to_csv(OUTPUT_PATH, index=False)\n",
        "\n",
        "print(f\"\\n✅ Submission file saved: {OUTPUT_PATH}\")\n",
        "print(sub.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evQEDGtU0Dzh",
        "outputId": "97066e29-eb57-4e67-c886-b5224d05d1ef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Submission file saved: submission.csv\n",
            "     Id  SalePrice\n",
            "0  1461  116537.23\n",
            "1  1462  158022.13\n",
            "2  1463  190392.18\n",
            "3  1464  196491.45\n",
            "4  1465  189646.66\n"
          ]
        }
      ]
    }
  ]
}